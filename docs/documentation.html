<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Analytical Chatbot - Technical Documentation</title>
    <!-- Prism.js for syntax highlighting (Tomorrow Night theme) -->
    <link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <style>
        :root {
            --bg-dark: #1a1a2e;
            --bg-card: #16213e;
            --accent: #4a9eff;
            --accent-dim: #2d5a8a;
            --text: #e0e0e0;
            --text-dim: #888;
            --code-bg: #0d1117;
            --success: #4caf50;
            --warning: #ff9800;
            --error: #f44336;
        }

        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            background: var(--bg-dark);
            color: var(--text);
            line-height: 1.6;
        }

        .container {
            max-width: 1100px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        header {
            text-align: center;
            padding: 60px 0;
            border-bottom: 1px solid rgba(255,255,255,0.1);
            margin-bottom: 40px;
        }

        header h1 {
            font-size: 2.5rem;
            margin-bottom: 10px;
            background: linear-gradient(135deg, #4a9eff, #a855f7);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        header p {
            color: var(--text-dim);
            font-size: 1.1rem;
        }

        nav {
            background: var(--bg-card);
            padding: 20px;
            border-radius: 12px;
            margin-bottom: 40px;
        }

        nav h2 {
            font-size: 0.9rem;
            text-transform: uppercase;
            letter-spacing: 1px;
            color: var(--text-dim);
            margin-bottom: 12px;
        }

        nav ul {
            list-style: none;
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
        }

        nav a {
            color: var(--accent);
            text-decoration: none;
            padding: 6px 12px;
            background: rgba(74, 158, 255, 0.1);
            border-radius: 6px;
            font-size: 0.9rem;
            transition: background 0.2s;
        }

        nav a:hover {
            background: rgba(74, 158, 255, 0.2);
        }

        section {
            margin-bottom: 60px;
        }

        h2 {
            font-size: 1.8rem;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--accent);
            display: inline-block;
        }

        h3 {
            font-size: 1.3rem;
            margin: 30px 0 15px;
            color: var(--accent);
        }

        h4 {
            font-size: 1.1rem;
            margin: 20px 0 10px;
            color: #a855f7;
        }

        p {
            margin-bottom: 15px;
            color: var(--text);
        }

        .card {
            background: var(--bg-card);
            border-radius: 12px;
            padding: 24px;
            margin: 20px 0;
        }

        .card-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .card h4 {
            margin-top: 0;
        }

        pre {
            background: var(--code-bg);
            border-radius: 8px;
            padding: 16px;
            overflow-x: auto;
            margin: 15px 0;
            border: 1px solid rgba(255,255,255,0.1);
        }

        code {
            font-family: 'SF Mono', 'Fira Code', 'Monaco', monospace;
            font-size: 0.9rem;
        }

        :not(pre) > code {
            background: rgba(74, 158, 255, 0.15);
            padding: 2px 6px;
            border-radius: 4px;
            color: var(--accent);
        }

        .diagram {
            background: var(--code-bg);
            border-radius: 12px;
            padding: 30px;
            margin: 20px 0;
            text-align: center;
            font-family: monospace;
            white-space: pre;
            overflow-x: auto;
            line-height: 1.4;
            color: var(--text-dim);
        }

        .diagram .highlight {
            color: var(--accent);
        }

        .diagram .node {
            color: #4caf50;
        }

        .diagram .flow {
            color: #ff9800;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid rgba(255,255,255,0.1);
        }

        th {
            background: var(--bg-card);
            color: var(--accent);
            font-weight: 600;
        }

        tr:hover {
            background: rgba(255,255,255,0.02);
        }

        .badge {
            display: inline-block;
            padding: 4px 10px;
            border-radius: 12px;
            font-size: 0.75rem;
            font-weight: 600;
            text-transform: uppercase;
        }

        .badge-blue { background: rgba(74, 158, 255, 0.2); color: var(--accent); }
        .badge-green { background: rgba(76, 175, 80, 0.2); color: var(--success); }
        .badge-orange { background: rgba(255, 152, 0, 0.2); color: var(--warning); }
        .badge-red { background: rgba(244, 67, 54, 0.2); color: var(--error); }

        .file-tree {
            font-family: monospace;
            background: var(--code-bg);
            padding: 20px;
            border-radius: 8px;
            line-height: 1.8;
            white-space: pre;
            overflow-x: auto;
        }

        .file-tree .folder { color: var(--accent); }
        .file-tree .file { color: var(--text-dim); }
        .file-tree .important { color: var(--success); }

        .callout {
            padding: 16px 20px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 4px solid;
        }

        .callout-info {
            background: rgba(74, 158, 255, 0.1);
            border-color: var(--accent);
        }

        .callout-warning {
            background: rgba(255, 152, 0, 0.1);
            border-color: var(--warning);
        }

        .callout-success {
            background: rgba(76, 175, 80, 0.1);
            border-color: var(--success);
        }

        .step {
            display: flex;
            gap: 20px;
            margin: 20px 0;
        }

        .step-number {
            width: 40px;
            height: 40px;
            background: var(--accent);
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            flex-shrink: 0;
        }

        .step-content {
            flex: 1;
        }

        .step-content h4 {
            margin-top: 0;
        }

        footer {
            text-align: center;
            padding: 40px 0;
            border-top: 1px solid rgba(255,255,255,0.1);
            color: var(--text-dim);
        }

        @media (max-width: 768px) {
            nav ul {
                flex-direction: column;
            }

            .step {
                flex-direction: column;
            }
        }

        /* Mermaid diagram styling */
        .mermaid {
            background: var(--code-bg);
            border-radius: 12px;
            padding: 30px;
            margin: 20px 0;
            text-align: center;
            overflow-x: auto;
        }

        /* Prism.js overrides for our theme */
        pre[class*="language-"],
        code[class*="language-"] {
            background: var(--code-bg) !important;
            font-size: 0.9rem !important;
            text-shadow: none !important;
        }

        pre[class*="language-"] {
            border-radius: 8px;
            border: 1px solid rgba(255,255,255,0.1);
            margin: 15px 0;
            padding: 16px !important;
        }

        .token.comment,
        .token.prolog,
        .token.doctype,
        .token.cdata {
            color: #6a9955 !important;
        }

        .token.keyword {
            color: #c586c0 !important;
        }

        .token.function {
            color: #dcdcaa !important;
        }

        .token.string {
            color: #ce9178 !important;
        }

        .token.number {
            color: #b5cea8 !important;
        }

        .token.operator {
            color: #d4d4d4 !important;
        }

        .token.class-name {
            color: #4ec9b0 !important;
        }

        .token.punctuation {
            color: #d4d4d4 !important;
        }

        .token.property {
            color: #9cdcfe !important;
        }

        .token.boolean {
            color: #569cd6 !important;
        }
    </style>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({
            startOnLoad: true,
            theme: 'dark',
            themeVariables: {
                primaryColor: '#4a9eff',
                primaryTextColor: '#e0e0e0',
                primaryBorderColor: '#4a9eff',
                lineColor: '#888',
                secondaryColor: '#16213e',
                tertiaryColor: '#1a1a2e',
                background: '#0d1117',
                mainBkg: '#16213e',
                nodeBorder: '#4a9eff',
                clusterBkg: '#1a1a2e',
                titleColor: '#e0e0e0',
                edgeLabelBackground: '#16213e'
            },
            flowchart: {
                curve: 'basis',
                htmlLabels: true
            }
        });
    </script>
</head>
<body>
    <div class="container">
        <header>
            <h1>Analytical Chatbot</h1>
            <p>Technical Documentation - Architecture, Design & Implementation</p>
        </header>

        <nav>
            <h2>Contents</h2>
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#architecture">Architecture</a></li>
                <li><a href="#pocketflow">PocketFlow</a></li>
                <li><a href="#nodes">Nodes</a></li>
                <li><a href="#flow">Flow</a></li>
                <li><a href="#sandbox">Sandbox</a></li>
                <li><a href="#backend">Backend API</a></li>
                <li><a href="#frontend">Frontend</a></li>
                <li><a href="#dataflow">Data Flow</a></li>
            </ul>
        </nav>

        <!-- Overview Section -->
        <section id="overview">
            <h2>Overview</h2>
            <p>
                The Analytical Chatbot is a web application that allows users to upload data files (CSV/JSON),
                ask questions in natural language, and receive answers powered by AI-generated Python code
                executed in a secure sandbox environment.
            </p>

            <div class="card-grid">
                <div class="card">
                    <h4>ü§ñ AI-Powered Analysis</h4>
                    <p>Uses OpenAI's GPT models to understand user queries and generate appropriate Python code for data analysis.</p>
                </div>
                <div class="card">
                    <h4>üîí Secure Execution</h4>
                    <p>Generated code runs in a sandboxed environment with restricted access - no file system, network, or dangerous operations.</p>
                </div>
                <div class="card">
                    <h4>üìä Modern Data Stack</h4>
                    <p>Uses Polars for fast DataFrame operations, DuckDB for SQL queries, and Altair for beautiful charts.</p>
                </div>
                <div class="card">
                    <h4>‚ö° Real-time Interface</h4>
                    <p>React frontend with FastAPI backend provides a responsive chat experience with instant feedback.</p>
                </div>
            </div>

            <h3>Project Structure</h3>
            <div class="file-tree">
<span class="folder">chatbot/</span>
‚îú‚îÄ‚îÄ <span class="important">api.py</span>              <span class="file"># FastAPI endpoints</span>
‚îú‚îÄ‚îÄ <span class="important">main.py</span>             <span class="file"># Entry point (uvicorn)</span>
‚îú‚îÄ‚îÄ <span class="important">nodes.py</span>            <span class="file"># PocketFlow node definitions</span>
‚îú‚îÄ‚îÄ <span class="important">flow.py</span>             <span class="file"># Flow orchestration</span>
‚îú‚îÄ‚îÄ <span class="folder">utils/</span>
‚îÇ   ‚îú‚îÄ‚îÄ <span class="important">call_llm.py</span>     <span class="file"># LLM wrapper with retry logic</span>
‚îÇ   ‚îú‚îÄ‚îÄ <span class="important">sandbox.py</span>      <span class="file"># Sandboxed code execution</span>
‚îÇ   ‚îî‚îÄ‚îÄ <span class="file">parse_code.py</span>   <span class="file"># Extract code from LLM response</span>
‚îú‚îÄ‚îÄ <span class="folder">frontend/</span>
‚îÇ   ‚îî‚îÄ‚îÄ <span class="folder">src/</span>
‚îÇ       ‚îú‚îÄ‚îÄ <span class="important">App.tsx</span>     <span class="file"># React application</span>
‚îÇ       ‚îî‚îÄ‚îÄ <span class="file">style.css</span>   <span class="file"># Styling</span>
‚îî‚îÄ‚îÄ <span class="folder">docs/</span>
    ‚îî‚îÄ‚îÄ <span class="file">design.md</span>       <span class="file"># Design document</span>
            </div>
        </section>

        <!-- Architecture Section -->
        <section id="architecture">
            <h2>Architecture</h2>
            <p>
                The application follows a client-server architecture with clear separation between
                the React frontend, FastAPI backend, and PocketFlow orchestration layer.
            </p>

            <div class="mermaid">
flowchart TB
    subgraph Browser["USER BROWSER"]
        subgraph Frontend["React Frontend (Vite)"]
            Upload["File Upload"]
            Chat["Chat Input"]
            Display["Message Display<br/>Text, Code, Plots"]
        end
    end

    subgraph Backend["FASTAPI BACKEND"]
        Endpoints["Endpoints: /chat, /upload, /files<br/>Session Management (cookie-based)"]
    end

    subgraph PocketFlow["POCKETFLOW ORCHESTRATION"]
        GetInput["GetInput"] --> Classify["ClassifyIntent"]
        Classify -->|conversation| ConvResp["ConversationResponse"]
        Classify -->|code_execution| GenCode["GenerateCode"]
        GenCode --> ExecCode["ExecuteCode"]
        ExecCode --> Format["FormatResults"]
        ConvResp --> Output["OutputResponse"]
        Format --> Output
    end

    subgraph External["EXTERNAL SERVICES"]
        OpenAI["OpenAI API<br/>(LLM Calls)"]
        Sandbox["Sandbox<br/>(Code Exec)"]
    end

    Frontend -->|HTTP JSON| Backend
    Backend -->|run_chatbot| PocketFlow
    PocketFlow --> OpenAI
    PocketFlow --> Sandbox
            </div>
        </section>

        <!-- PocketFlow Section -->
        <section id="pocketflow">
            <h2>PocketFlow Framework</h2>
            <p>
                PocketFlow is a minimalist (~100 lines) LLM orchestration framework. It provides two core abstractions:
            </p>

            <div class="card-grid">
                <div class="card">
                    <h4>Node</h4>
                    <p>The smallest unit of work. Each node has three lifecycle methods:</p>
                    <ul>
                        <li><code>prep(shared)</code> - Read data from shared store</li>
                        <li><code>exec(prep_res)</code> - Execute computation (e.g., LLM call)</li>
                        <li><code>post(shared, prep_res, exec_res)</code> - Write results, return action</li>
                    </ul>
                </div>
                <div class="card">
                    <h4>Flow</h4>
                    <p>Orchestrates nodes into a directed graph:</p>
                    <ul>
                        <li>Nodes connected with <code>>></code> operator</li>
                        <li>Conditional branching with <code>- "action" >></code></li>
                        <li>Shared store enables node communication</li>
                    </ul>
                </div>
            </div>

            <h3>Shared Store Pattern</h3>
            <p>
                All nodes communicate through a shared dictionary. This separates data (what) from logic (how):
            </p>
            <pre><code class="language-python">shared = {
    # Input
    "user_message": "Create a histogram of ages",
    "uploaded_files": {"titanic.csv": pl.DataFrame(...)},  # Polars DataFrame
    "chat_history": [...],

    # Processing (populated by nodes)
    "intent": "code_execution",
    "generated_code": "chart = alt.Chart(df)...",
    "execution_result": {"stdout": "...", "plots": [...]},

    # Output
    "response": {
        "message": "Here's the histogram...",
        "code": "...",
        "output": "...",
        "plots": ["base64..."]
    }
}</code></pre>

            <h3>Key Design Principles</h3>
            <div class="card-grid">
                <div class="card">
                    <h4>Minimalism</h4>
                    <p>PocketFlow is designed to be lightweight (~100 lines of core code) with zero external dependencies, making it easy to understand and extend.</p>
                </div>
                <div class="card">
                    <h4>Separation of Concerns</h4>
                    <p>Data (Shared Store) is strictly separated from Compute (Node execution). Nodes read input, process it, and write output without managing state directly.</p>
                </div>
            </div>

            <h3>Batch Processing</h3>
            <p>
                PocketFlow supports processing large datasets efficiently through specialized batch abstractions:
            </p>
            <ul>
                <li><strong>BatchNode</strong>: Splits input into chunks (e.g., file rows, text segments) and processes them iteratively.</li>
                <li><strong>BatchFlow</strong>: Runs an entire Flow repeatedly for a list of items (e.g., processing multiple files).</li>
            </ul>
            <p>This allows for scalable patterns like <strong>Map-Reduce</strong> where a large task is broken down into parallelizable sub-tasks.</p>

            <h3>Node Lifecycle</h3>
            <div class="mermaid">
flowchart TD
    Prep["<b>prep(shared)</b><br/>Read from store"]
    Exec["<b>exec(prep_res)</b><br/>Do computation<br/>(LLM call, etc)<br/>May retry on error"]
    Post["<b>post(shared, ...)</b><br/>Write to store<br/>Return action"]
    Next["Next node based on<br/>action string"]

    Prep -->|prep_res| Exec
    Exec -->|exec_res| Post
    Post -->|action| Next
            </div>
        </section>

        <!-- Nodes Section -->
        <section id="nodes">
            <h2>Node Definitions</h2>
            <p>
                The chatbot is built from 7 specialized nodes, each handling a specific part of the pipeline.
                All nodes are defined in <code>nodes.py</code>.
            </p>

            <table>
                <thead>
                    <tr>
                        <th>Node</th>
                        <th>Purpose</th>
                        <th>Key Actions</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><code>GetInputNode</code></td>
                        <td>Initialize processing context</td>
                        <td>Validates input, returns "default" or "output"</td>
                    </tr>
                    <tr>
                        <td><code>ClassifyIntentNode</code></td>
                        <td>Determine user intent via LLM</td>
                        <td>Returns "conversation" or "code_execution"</td>
                    </tr>
                    <tr>
                        <td><code>ConversationResponseNode</code></td>
                        <td>Generate chat response</td>
                        <td>For non-analytical queries</td>
                    </tr>
                    <tr>
                        <td><code>GenerateCodeNode</code></td>
                        <td>Generate Python code via LLM</td>
                        <td>Creates Altair/pandas code</td>
                    </tr>
                    <tr>
                        <td><code>ExecuteCodeNode</code></td>
                        <td>Run code in sandbox</td>
                        <td>Captures output and plots</td>
                    </tr>
                    <tr>
                        <td><code>FormatResultsNode</code></td>
                        <td>Format output for display</td>
                        <td>Generates explanation of results</td>
                    </tr>
                    <tr>
                        <td><code>OutputResponseNode</code></td>
                        <td>Finalize and store response</td>
                        <td>Updates chat history</td>
                    </tr>
                </tbody>
            </table>

            <h3>ClassifyIntentNode</h3>
            <p>This node uses an LLM to classify whether the user wants conversation or data analysis:</p>
            <pre><code class="language-python">class ClassifyIntentNode(Node):
    def prep(self, shared):
        # Gather context: message, history, file info
        return {
            "message": shared["user_message"],
            "history": shared["chat_history"][-6:],
            "files": [f"{fname}: {df.shape}" for fname, df in shared["uploaded_files"].items()]
        }

    def exec(self, prep_res):
        prompt = f"""Classify the user's intent...
        Current message: {prep_res["message"]}

        Respond in YAML:
        ```yaml
        intent: conversation | code_execution
        reason: brief explanation
        ```"""

        # Note: ConversationResponseNode can now generate Mermaid diagrams
        # by wrapping code in ```mermaid blocks.

        response = call_llm(prompt)
        result = yaml.safe_load(extract_yaml(response))

        assert result["intent"] in ["conversation", "code_execution"]
        return result

    def post(self, shared, prep_res, exec_res):
        shared["intent"] = exec_res["intent"]
        return exec_res["intent"]  # This determines next node!</code></pre>

            <h3>GenerateCodeNode</h3>
            <p>Generates Python code tailored for the user's analytical request:</p>
            <pre><code class="language-python">class GenerateCodeNode(Node):
    def exec(self, prep_res):
        system_prompt = """You are a Python code generator for data analysis.
        - PREFER Polars (as pl) over Pandas - it's faster and more expressive
        - Uses Altair (as alt) for ALL visualizations
        - Use show_table(df) for nicely formatted tables
        - Use show_html(str) for interactive dashboards
        - Polars syntax: df.filter(pl.col('age') > 30)
        """

        prompt = f"""Generate Python code for: {prep_res["message"]}
        Available DataFrames (Polars): {prep_res["file_vars"]}
        """

        response = call_llm(prompt, system_prompt)
        return parse_code_block(response)</code></pre>

            <div class="callout callout-info">
                <strong>Retry Logic:</strong> Nodes can specify <code>max_retries</code> and <code>wait</code>
                parameters. If <code>exec()</code> raises an exception, PocketFlow automatically retries.
            </div>
        </section>

        <!-- Flow Section -->
        <section id="flow">
            <h2>Flow Orchestration</h2>
            <p>
                The flow connects nodes and defines the execution path based on actions returned by each node.
                Defined in <code>flow.py</code>.
            </p>

            <pre><code class="language-python">def create_chatbot_flow() -> Flow:
    # Create nodes with retry configuration
    get_input = GetInputNode()
    classify_intent = ClassifyIntentNode(max_retries=3, wait=1)
    conversation_response = ConversationResponseNode(max_retries=2)
    generate_code = GenerateCodeNode(max_retries=3)
    execute_code = ExecuteCodeNode()
    format_results = FormatResultsNode(max_retries=2)
    output_response = OutputResponseNode()

    # Wire the flow with >> operator
    get_input >> classify_intent

    # Branching based on intent
    classify_intent - "conversation" >> conversation_response
    classify_intent - "code_execution" >> generate_code

    # Conversation path
    conversation_response - "output" >> output_response

    # Code execution path
    generate_code >> execute_code >> format_results >> output_response

    return Flow(start=get_input)</code></pre>

            <h3>Flow Diagram</h3>
            <div class="mermaid">
flowchart TD
    GetInput["GetInput"]
    Classify["ClassifyIntent"]
    ConvResp["ConversationResponse"]
    GenCode["GenerateCode"]
    ExecCode["ExecuteCode"]
    Format["FormatResults"]
    Output["OutputResponse"]

    GetInput --> Classify
    Classify -->|"conversation"| ConvResp
    Classify -->|"code_execution"| GenCode
    GenCode --> ExecCode
    ExecCode --> Format
    ConvResp --> Output
    Format --> Output
            </div>
        </section>

        <!-- Sandbox Section -->
        <section id="sandbox">
            <h2>Sandboxed Code Execution</h2>
            <p>
                The sandbox (<code>utils/sandbox.py</code>) executes user-generated code safely with multiple
                layers of protection.
            </p>

            <h3>Security Layers</h3>
            <div class="card-grid">
                <div class="card">
                    <h4>üö´ Forbidden Patterns</h4>
                    <p>Code is scanned for dangerous patterns before execution:</p>
                    <pre><code class="language-python">FORBIDDEN = [
    'import os',
    'import subprocess',
    '__import__',
    'exec(', 'eval(',
    'open(', 'file(',
    '__builtins__',
    'getattr', 'setattr'
]</code></pre>
                </div>
                <div class="card">
                    <h4>‚úÖ Safe Builtins</h4>
                    <p>Only whitelisted built-in functions are available:</p>
                    <pre><code class="language-python">SAFE_BUILTINS = {
    'len', 'range', 'list',
    'dict', 'set', 'tuple',
    'int', 'float', 'str',
    'max', 'min', 'sum',
    'sorted', 'enumerate',
    'zip', 'map', 'filter',
    'print', 'isinstance'
}</code></pre>
                </div>
                <div class="card">
                    <h4>üì¶ Allowed Modules</h4>
                    <p>Pre-imported safe modules for data analysis:</p>
                    <pre><code class="language-python">namespace = {
    'pl': polars,
    'pd': pandas,
    'np': numpy,
    'alt': altair,
    'query_db': func,  # SQL queries
    'show_table': func, # HTML table display
    'show_html': func,  # Dashboard display
    'math': math,
    'statistics': statistics,
    'json': json
}</code></pre>
                </div>
                <div class="card">
                    <h4>‚è±Ô∏è Execution Timeout</h4>
                    <p>Code runs in a daemon thread with timeout:</p>
                    <pre><code class="language-python">thread = Thread(target=run, daemon=True)
thread.start()
thread.join(timeout=30)

if thread.is_alive():
    return {"error": "Timeout"}</code></pre>
                </div>
            </div>

            <h3>Execution Flow</h3>
            <pre><code class="language-python">def execute_sandboxed_code(code: str, context: dict, timeout: int = 30) -> dict:
    # 1. Safety check - scan for forbidden patterns
    is_safe, error = check_code_safety(code)
    if not is_safe:
        return {"success": False, "error": error}

    # 2. Build namespace with safe builtins + modules + user data
    namespace = _build_namespace(context)  # Includes uploaded DataFrames

    # 3. Execute in thread with timeout
    stdout_buffer = io.StringIO()
    thread_result = {"completed": False, "exception": None}

    def run_code():
        with redirect_stdout(stdout_buffer):
            exec(compile(code, '&lt;sandbox&gt;', 'exec'), namespace)
        thread_result["completed"] = True

    thread = Thread(target=run_code, daemon=True)
    thread.start()
    thread.join(timeout=timeout)

    # 4. Capture Altair charts as PNG
    plots = []
    if HAS_ALTAIR:
        for name, val in namespace.items():
            if isinstance(val, alt.TopLevelMixin):
                png = vl_convert.vegalite_to_png(val.to_dict())
                plots.append(base64.b64encode(png).decode())

    return {
        "success": True,
        "stdout": stdout_buffer.getvalue(),
        "plots": plots
    }</code></pre>

            <div class="callout callout-warning">
                <strong>Note:</strong> While Python threads cannot be forcefully killed, the daemon thread
                will be abandoned on timeout. The 30-second limit prevents runaway computations.
            </div>
        </section>

        <!-- Backend Section -->
        <section id="backend">
            <h2>Backend API</h2>
            <p>
                The FastAPI backend (<code>api.py</code>) provides REST endpoints for the frontend
                and manages user sessions.
            </p>

            <h3>Endpoints</h3>
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Endpoint</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><span class="badge badge-green">GET</span></td>
                        <td><code>/</code></td>
                        <td>Health check</td>
                    </tr>
                    <tr>
                        <td><span class="badge badge-blue">POST</span></td>
                        <td><code>/chat</code></td>
                        <td>Send message, receive AI response</td>
                    </tr>
                    <tr>
                        <td><span class="badge badge-blue">POST</span></td>
                        <td><code>/upload</code></td>
                        <td>Upload CSV/JSON file</td>
                    </tr>
                    <tr>
                        <td><span class="badge badge-green">GET</span></td>
                        <td><code>/files</code></td>
                        <td>List uploaded files</td>
                    </tr>
                    <tr>
                        <td><span class="badge badge-red">DELETE</span></td>
                        <td><code>/upload/{filename}</code></td>
                        <td>Remove uploaded file</td>
                    </tr>
                    <tr>
                        <td><span class="badge badge-blue">POST</span></td>
                        <td><code>/clear</code></td>
                        <td>Clear session (files + history)</td>
                    </tr>
                </tbody>
            </table>

            <h3>Session Management</h3>
            <pre><code class="language-python"># In-memory session store
session_store: Dict[str, Dict] = {}
# Structure: { session_id: { "files": {...}, "history": [...] } }

def get_session_id(request: Request, response: Response) -> str:
    session_id = request.cookies.get("session_id")
    if not session_id:
        session_id = str(uuid.uuid4())
        response.set_cookie("session_id", session_id, httponly=True)

    if session_id not in session_store:
        session_store[session_id] = {"files": {}, "history": []}

    return session_id</code></pre>

            <h3>Chat Endpoint</h3>
            <pre><code class="language-python">@app.post("/chat")
def chat_endpoint(req: ChatRequest, session_id: str = Depends(get_session_id)):
    session = session_store[session_id]

    # Run PocketFlow pipeline
    result = run_chatbot(
        user_message=req.message,
        uploaded_files=session["files"],    # Dict of filename -> DataFrame
        chat_history=session["history"]      # List of {role, content}
    )

    # Update history
    session["history"] = result["chat_history"]

    return {"response": result["response"]}</code></pre>

            <h3>Response Format</h3>
            <pre><code class="language-json">{
    "response": {
        "message": "The average age is 29.7 years.",
        "code": "avg_age = titanic['Age'].mean()\nprint(f'Average: {avg_age:.1f}')",
        "output": "Average: 29.7",
        "plots": ["iVBORw0KGgo..."],
        "tables": ["&lt;table&gt;...&lt;/table&gt;"],
        "html": ["&lt;!DOCTYPE html&gt;..."],
        "error": null
    }
}</code></pre>
        </section>

        <!-- Frontend Section -->
        <section id="frontend">
            <h2>Frontend (React)</h2>
            <p>
                The frontend (<code>frontend/src/App.tsx</code>) is a React application built with Vite
                that provides the chat interface.
            </p>

            <h3>Key Components</h3>
            <pre><code class="language-typescript">// Message type definition
interface ChatResponse {
    message: string
    code?: string | null
    output?: string | null
    plots?: string[] | null
    tables?: string[] | null
    html?: string[] | null  // Interactive HTML dashboards
    error?: string | null
}

interface Message {
    role: 'user' | 'assistant'
    content: string
    response?: ChatResponse  // Full response for assistant messages
}</code></pre>

            <h3>Mermaid Diagram Rendering</h3>
            <pre><code class="language-tsx">import mermaid from 'mermaid'

// Initialize mermaid
mermaid.initialize({ startOnLoad: false, theme: 'dark' })

function MermaidChart({ chart }: { chart: string }) {
    const [svg, setSvg] = useState('')
    const id = useRef(`mermaid-${Math.random().toString(36)}`).current

    useEffect(() => {
        mermaid.render(id, chart).then(({ svg }) => setSvg(svg))
    }, [chart])

    return &lt;div dangerouslySetInnerHTML={{ __html: svg }} /&gt;
}</code></pre>

            <h3>Collapsible Code Block</h3>
            <pre><code class="language-tsx">function CodeBlock({ code }: { code: string }) {
    const [expanded, setExpanded] = useState(false)
    const lineCount = code.split('\n').length

    return (
        &lt;div className={`code-block ${expanded ? 'expanded' : 'collapsed'}`}&gt;
            &lt;div className="code-header" onClick={() => setExpanded(!expanded)}&gt;
                &lt;span&gt;{expanded ? '‚ñº' : '‚ñ∂'} Python ({lineCount} lines)&lt;/span&gt;
                &lt;button onClick={(e) => {
                    e.stopPropagation()
                    navigator.clipboard.writeText(code)
                }}&gt;Copy&lt;/button&gt;
            &lt;/div&gt;
            {expanded && &lt;pre&gt;&lt;code&gt;{code}&lt;/code&gt;&lt;/pre&gt;}
        &lt;/div&gt;
    )
}</code></pre>

            <h3>Message Rendering</h3>
            <pre><code class="language-tsx">import ReactMarkdown from 'react-markdown'

function renderMessage(msg: Message) {
    const resp = msg.response

    return (
        &lt;div className="msg assistant"&gt;
            {/* Markdown rendered text with Mermaid support */}
            {msg.content && (
                &lt;ReactMarkdown
                    components={{
                        code({ className, children }) {
                            const match = /language-mermaid/.exec(className || '')
                            return match ? 
                                &lt;MermaidChart chart={String(children)} /&gt; : 
                                &lt;code className={className}&gt;{children}&lt;/code&gt;
                        }
                    }}
                &gt;
                    {msg.content}
                &lt;/ReactMarkdown&gt;
            )}

            {/* Collapsible code */}
            {resp?.code && &lt;CodeBlock code={resp.code} /&gt;}

            {/* Output block */}
            {resp?.output && &lt;pre className="output"&gt;{resp.output}&lt;/pre&gt;}

            {/* Rendered plots */}
            {resp?.plots?.map((plot, i) => (
                &lt;img
                    key={i}
                    src={`data:image/png;base64,${plot}`}
                    alt={`Plot ${i+1}`}
                /&gt;
            ))}

            {/* Interactive Dashboards */}
            {resp?.html?.map((html, i) => (
                &lt;iframe
                    key={i}
                    srcDoc={html}
                    title={`Dashboard ${i+1}`}
                    sandbox="allow-scripts"
                /&gt;
            ))}

            {/* Error display */}
            {resp?.error && &lt;div className="error"&gt;{resp.error}&lt;/div&gt;}
        &lt;/div&gt;
    )
}</code></pre>

            <h3>API Communication</h3>
            <pre><code class="language-typescript">async function sendMessage() {
    const userMsg = { role: 'user', content: input }
    setMessages(m => [...m, userMsg])
    setLoading(true)

    const res = await fetch(`${API_BASE}/chat`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ message: input }),
        credentials: 'include'  // Send session cookie
    })

    const data = await res.json()
    const response = data.response as ChatResponse

    setMessages(m => [...m, {
        role: 'assistant',
        content: response.message,
        response  // Store full response for rendering
    }])
}</code></pre>
        </section>

        <!-- Data Flow Section -->
        <section id="dataflow">
            <h2>Complete Data Flow</h2>
            <p>
                Here's a step-by-step walkthrough of what happens when a user asks
                "Create a histogram of passenger ages" with a Titanic dataset uploaded:
            </p>

            <div class="step">
                <div class="step-number">1</div>
                <div class="step-content">
                    <h4>User Sends Message</h4>
                    <p>Frontend sends POST to <code>/chat</code> with session cookie:</p>
                    <pre><code class="language-json">{ "message": "Create a histogram of passenger ages" }</code></pre>
                </div>
            </div>

            <div class="step">
                <div class="step-number">2</div>
                <div class="step-content">
                    <h4>Backend Receives Request</h4>
                    <p>FastAPI extracts session, retrieves uploaded files (DataFrames), and calls <code>run_chatbot()</code>:</p>
                    <pre><code class="language-python">result = run_chatbot(
    user_message="Create a histogram of passenger ages",
    uploaded_files={"titanic.csv": DataFrame(...)},
    chat_history=[...]
)</code></pre>
                </div>
            </div>

            <div class="step">
                <div class="step-number">3</div>
                <div class="step-content">
                    <h4>ClassifyIntentNode</h4>
                    <p>LLM determines this is a <code>code_execution</code> request (wants visualization).</p>
                    <p>Returns action: <code>"code_execution"</code> ‚Üí flow proceeds to GenerateCodeNode</p>
                </div>
            </div>

            <div class="step">
                <div class="step-number">4</div>
                <div class="step-content">
                    <h4>GenerateCodeNode</h4>
                    <p>LLM generates Altair code based on the request and data schema:</p>
                    <pre><code class="language-python"># Generated code (using Polars DataFrame)
# Altair works directly with Polars DataFrames
chart = alt.Chart(titanic).mark_bar().encode(
    alt.X('Age:Q', bin=alt.Bin(maxbins=30), title='Age'),
    y='count()'
).properties(
    title='Distribution of Passenger Ages',
    width=600,
    height=400
)
print("Histogram created")</code></pre>
                </div>
            </div>

            <div class="step">
                <div class="step-number">5</div>
                <div class="step-content">
                    <h4>ExecuteCodeNode</h4>
                    <p>Code runs in sandbox with <code>titanic</code> DataFrame pre-loaded.
                       The Altair chart is captured and converted to PNG via <code>vl-convert</code>.</p>
                    <pre><code class="language-python">execution_result = {
    "stdout": "Histogram created\n",
    "plots": ["iVBORw0KGgo..."],  # Base64 PNG
    "success": True
}</code></pre>
                </div>
            </div>

            <div class="step">
                <div class="step-number">6</div>
                <div class="step-content">
                    <h4>FormatResultsNode</h4>
                    <p>LLM generates a natural language explanation of the results:</p>
                    <pre><code class="language-python">response = {
    "message": "Here's a histogram showing the age distribution of Titanic passengers. "
               "The data shows most passengers were between 20-40 years old.",
    "code": "chart = alt.Chart...",
    "output": "Histogram created",
    "plots": ["iVBORw0KGgo..."]
}</code></pre>
                </div>
            </div>

            <div class="step">
                <div class="step-number">7</div>
                <div class="step-content">
                    <h4>Frontend Renders Response</h4>
                    <p>React displays the message, collapsible code block, output, and the histogram image:</p>
                    <pre><code class="language-jsx">&lt;p&gt;Here's a histogram showing the age distribution...&lt;/p&gt;
&lt;CodeBlock code="chart = alt.Chart..." /&gt;
&lt;pre class="output"&gt;Histogram created&lt;/pre&gt;
&lt;img src="data:image/png;base64,iVBORw0KGgo..." /&gt;</code></pre>
                </div>
            </div>

            <div class="callout callout-success">
                <strong>Result:</strong> The user sees a friendly explanation, can optionally expand the code,
                and views the generated histogram - all from a single natural language request!
            </div>
        </section>

        <footer>
            <p>Analytical Chatbot Documentation ‚Ä¢ Built with PocketFlow, FastAPI, React, Polars, DuckDB, and Altair</p>
        </footer>
    </div>

    <!-- Prism.js core and language support -->
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-javascript.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-typescript.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-json.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-yaml.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-jsx.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-tsx.min.js"></script>
</body>
</html>
